\section{Solution}
For every solution we took the following approach:
The sorted input arrays $A$ and $B$ are split into parts
$A = A_1 \concat A_2 \concat \ldots \concat A_p$  and
$B = B_1 \concat B_2 \concat \ldots \concat B_p$
, in a way that all elements in $A_i \cup B_i$ are smaller then those in $A_{i+1} \cup B_{i+1}$. 
With such a distribution given, each Process $i$ merges independently $A_i$ and $B_i$ using a simple sequential merge algorithm.
Afterwards a master process concatinates the results.
The sequential merge function is in following noted by $\merge()$.

\subsection{Co-ranking}
In order to obtain a distribution as mentioned above, we implemented a co-ranking algorithm.
The idea is that for a given index $i$ of the resulting array $C$, 
the co-ranking algorithm $\corank(i)$ should yield a pair of indexes $(j,k)$
such that
\begin{equation}\label{eq:coranking1}
  C[0\ldots i-1] = \merge(A[0\ldots j-1],B[0\ldots k-1]).
\end{equation}
The impact for parallelization is the following:
Let $i$ be the id of a process,
then we make this process calculate the entries of $C[i \cdot l, \ldots, (i+1)\cdot l - 1]$,
where $l$ is the blocksize that each process calculates.
Therefore it uses the coranking algorithm to get
$(j_1,k_1) = \corank(i\cdot l)$ and $(j_2,k_2) = \corank((i+1)\cdot l)$.
With \eqref{eq:coranking1} one has
\begin{align*}
  \merge(A[j_1,\ldots,j_2],B[k_1,\ldots,k_2]) = C[i \cdot l,\ldots, (i+1)l - 1].
\end{align*}
So each process can independently both calculate the coranks and perform the merge.

To calculate the coranks we use that
$j$ and $k$, which fulfill \eqref{eq:coranking1}, are also given by the following
properties\footnote{As shown in Lemma1 in Siebert, Träff: Perfectly load-balanced, optimal, stable, parallel merge (https://arxiv.org/abs/1303.4312)}
\begin{enumerate}
  \item $j + k = i$
  \item $j = 0 \vee A[j-1] \leq B[k]$\label{first_inequal}
  \item $k = 0 \vee B[k-1] < A[j]$
\end{enumerate}
Tith leads to a binary-like search for the co-ranks.
For $j$ take an arbitrary value $\leq m$ and set $k$ so that $k + j = i$.
Also keep track of the lower limits for $j$ and $k$ in $j_{min}$ and $k_{min}$.

Now check:
If $j=0 \vee A[j-1] > B[k]$, then point 2 is violated so $j$ was too great and $k$ too small,
so set $k_{low}=k$ and try again with $j = \floor*{\middlee(j,j_{low})}$ and $k$ adjusted such that $j + k = i$.

Or if $k=0 \vee B[k-1] \geq A[j]$, then point 1 is violated and $j$ was too small.
So set $j_{low}=j$ and try again with $k = \floor*{\middlee(k,k_{low})}$ and $j$ adjusted according to point 1.

It can be proven that this infact terminates and computes the co-ranks to $i$\footnote{Proposition1 of the same source by Siebert and Träff}.

%\lstinputlisting[caption=co-ranking algorithm in C,label=code:corank,style=c]{corank.c}
%\begin{lstlisting}[caption=co-ranking algorithm in C,label=code:corank,style=c]
 % while(counter++ < (m + n)) {
  %  if( j>0  &&  k<n  &&  A[j-1]>B[k] ) {
  %    k_low = k;
  %    int diff = j-j_low, delta = diff/2 + (diff&1);
  %    j = j - delta;
  %}
%\end{lstlisting}


\subsection{Implementation}
Every implementation uses the following structure:
\begin{enumerate}
  \item read the input
  \item merge in parallel using co-ranks
  \item put the parts together
  \item check if result is ordered again
\end{enumerate}
The details for each implementation are presented in the following paragraphs.

\subsubsection{OpenMP}
In OpenMP, the programm runs sequentially in one master thread until it reaches a parallel section.
At this point new threads are created to execute this section simultaneously.
This section is marked by the directive

\textbf{\#pragma omp parallel num\_threads(p) private(a, b, ..)},

where p is the number of threads and a, b, ... are local variables.
All threads share a common address space.
If a variable is not explicitly marked as private, all threads access the same address when they use this variable.
When the parallel section is left, join ... master thread runs again.

For our implementation this means, that the master thread first reads/creates the input arrays.
Afterwards it allocates the space for the resulting array and makes it available in a public variable.
Then the parallel section is entered and the threads are created.
Each thread calculates its coranks for the indexes
$id \cdot \frac{output\_length}{number\_of\_threads}$
and
$(id+1)\cdot \frac{output\_length}{number\_of\_threads}$, where $id$ is the id of the actual thread.
Then it merges directly from the input arrays into the space that is allocated for the result.

Note here that both the output array and the input arrays are public variables for all threads.
Then the parallel segment is left and the master thread checks the result for corectness.


\subsubsection{Cilk}
Cilk works different than OpenMP. In Cilk we have p worker threads that work on any amount of tasks. So our the solution is to break the arrays down into subparts by a recursive strategy, which can be seen in Listing \ref{code:cilk_recursion}.

In our first solution the \textit{target\_size}, which specifies the maximum amount of elements a task can have, was an constant of 4096. The problem was that too many packets were generated and the stealing of packets took much longer than the sequential algorithm.

So we changed the constant to a dynamic variable that is calculated so only 48 packets will be created. So we try to remove the work-stealing part of cilk away, as it seems too slow an does not benefit our runtime in any way. The idea behind our algorithm is to provide enough packets for systems with up to 48 worker threads, as the biggest machine we tested our algorithm on had that many cores.
But there is another reason why we switched to this fixed amount of tasks solution. If we would have kept the old solution with a constant cutoff value, we would have needed to increase this value to a rather large number, to justify the generation of a packet. This would result in no paralellization to be used for the problemsizes we want to test as 50-100000 would have ended up being only one packet.

After we split up the problem with this recursive algorithm we finally calculated the coranks for each packet and started to merge each packet sequentially.

\begin{lstlisting}[caption=recursive cilk strategy, label=code:cilk_recursion, style=c]
void cilk_mergeRecursive(struct merge_sample *sample, INPUTTYPE *output, int range1, int range2, int target_size) {
	int size = range2 - range1 + 1;
	
	if(size <= target_size) {
		merge_log("Starting sequential mode for [%d; %d] (size=%d)", range1, range2, size);
		cilk_mergeSeq(sample, output, range1, range2);
		return;
	}
	
	int middle = range1 + size / 2;
	merge_log("Splitting in [%d; %d] and [%d; %d]", range1, middle - 1, middle, range2);
	
	cilk_spawn cilk_mergeRecursive(sample, output, range1, middle - 1, target_size);
	cilk_spawn cilk_mergeRecursive(sample, output, middle, range2, target_size);
}
\end{lstlisting}

\subsubsection{MPI}
For the MPI implementation it is assumed that the input arrays are already distributed among all processes.
In contrast to other implementation these processes need to activly communicate to calculate the coranks.
To archieve this, each process makes its array parts accessible by opening a MPI window.
This is demonstrated in \hyperref[code:mpi_window]{Listing \ref*{code:mpi_window}}.

\begin{lstlisting}[caption=window to share arrays, label=code:mpi_window,style=c]
MPI_Win_create(A, len1, sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &winA);
MPI_Win_create(B, len2, sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &winB);
\end{lstlisting}

Afterwards other processes can read from them as presented in \hyperref[code:mpi_get]{Listing \ref*{code:mpi_get}}.

Eventually we wrote a function getValueFrom(int index,MPI\_Win win,...) for reading elements from a certain position in $A$ (or $B$).
Therefor it calculates which process has the required part of $A$ (or $B$).
If it is the same process it reads it from its own array.
Otherwise it uses the access method from \hyperref[code:mpi_get]{Listing \ref*{code:mpi_get}}.

\begin{lstlisting}[caption=window to share arrays, label=code:mpi_get, style=c]
int value;
MPI_Win_lock\(MPI_LOCK_SHARED, targetRank, 0, win1\);
  MPI_Get\(value, 1, MPI_INT, targetRank, d, 1, MPI_INT, win1\);
MPI_Win_unlock\(targetRank, win1\);
\end{lstlisting}

For a MPI version of the corank algorithm, we replaced all occurrences of $A[i]$ and $B[i]$ with theValueFrom()-function.


\textbf{Problems with MPI:}
At the time of writing we had problems to run our MPI implementation with arbitrary array sizes.
So we assume that the number of processes divides the array lengths and that both arrays have the same size.

Unfortunately, our MPI solution only supports INT-arrays.

\section{Testing}
The coranking algorithm was tested more consequently as it is used by every implementation for each framework. We wrote a program that tested the functionality for normal cases and edge cases like arrays with only one element or arrays with duplicate numbers.

We tested every implementation with the following scenarios:
\begin{enumerate}
  \item all elements in the first array are smaller than these in the second array
  \item all elements in the first array are greater than these in the second array
  \item the numberes are distributed randomly, but arrays are still ordered
\end{enumerate}
In addition, the resulting array is always checked for correctness as part of the implementation.
During the complete benchmarking process, the code was therefore tested automatically.


