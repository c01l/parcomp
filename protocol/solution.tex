\section{Solution}

For every solution we took the following approach:
The sorted input arrays $A$ and $B$ are split into parts
$A = A_1 \concat A_2 \concat \ldots \concat A_p$  and
$B = B_1 \concat B_2 \concat \ldots \concat B_p$
, in a way that all elements in $A_i \cup B_i$ are smaller then those in $A_{i+1} \cup B_{i+1}$. 
With such a distribution given, each Process $i$ merges independently $A_i$ and $B_i$ using a simple sequential merge algorithm.
Afterwards a master process concatinates the results.

\subsection{Co-ranking}
In order to obtain a distribution as mentioned above, we implemented a co-ranking algorithm.
%The idea was that the resulting array is divided in as equal ranges as possible.
%For each range we are interested in the elements of $A$ and $B$ which will go there.
%Since the input arrays are already sorted, these elements are positioned in a row within $A$.
%So the co-ranking algorithm should yield the indexes of $A$ and $B$ that frame such a block.
The idea is that for a given index $i$ of the resulting array $C$, 
the co-ranking algorithm should yield us indexes $j$ and $k$
%such that $j+k=l$ and $A[i] < C[l]$ for all $i < j$ and $B[i'] < C[l]$ for all $i' < k$.
such that
\begin{equation}\label{eq:coranking1}
  C[0\ldots i-1] = \merge(A[0\ldots j-1],B[0\ldots k-1]).
\end{equation}
%The indexes $j$ and $k$ are called the coranks of $A$ and $B$, respectively.
During the parallelization, the $i$'th process should calculate $C[i \cdot l, \ldots, (i+1) l - 1]$,
where $l$ is the blocksize.
Therefore we use the coranking algorithm to get indexes $j_1, j_2, k_1, k_2$ by
$(j_1,k_1) = \corank(i\cdot l)$ and $(j_2,k_2) = \corank((i+1)\cdot l)$.
Then it holds that
\begin{align*}
  \merge(A[j_1,\ldots,j_2],B[k_1,\ldots,k_2]) = C[i \cdot l,\ldots, (i+1)l - 1].
\end{align*}
So each process can independently both calculate the coranks and perform the merge.

To calculate the coranks we use that
$j$ and $k$, which fulfill \eqref{eq:coranking1}, are also given by the following
properties\footnote{As shown in https://arxiv.org/abs/1303.4312}
\begin{enumerate}
  \item $j + k = i$
  \item $j = 0 \vee A[j-1] \leq B[k]$
  \item $k = 0 \vee B[k-1] < A[j]$
\end{enumerate}
This leads to following implementation in C.
\lstinputlisting[caption=co-ranking algorithm in C,label=code:corank,style=c]{corank.c}


\subsection{Implementation}

\subsubsection{OpenMP}
In OpenMP, the programm runs sequentially in one master thread until it reaches a parallel section.
At this point new threads are created to execute this section simultaneously.
This section is marked by the directive \textbf{pragma omp parallel num\_threads(p) private(a, b, ..)}, where p is the number of threads and a, b, ... are local variables.
All threads share a common address space.
If a variable is not explitly marked as private, all threads access the same address when they use this variable.
When the parallel section is left, join ... master thread runs again.

For our implementation this means, that the reading or the creation of the input arrays are first done sequentially in a master thread.
Space for the result array is allocated.
Then the section is entered.
Each thread calculates
•its ranges
$\big(id \cdot \frac{output\_length}{threads}, (id+1)\frac{output\_length}{threads}\big)$
• the coranks

and merges directly in the space that is allocated for the output.



\subsubsection{Cilk}

\subsubsection{MPI}
For the MPI implementation it is assumed that the input arrays are already distributed among all processes.
In contrast to other implementation these processes need to activly communicate to calculate the coranks.
To archieve this, each process makes its array parts accessible by opening a MPI window.
This is demonstrated in \hyperref[code:mpi_window]{Listing \ref*{code:mpi_window}}.

\begin{lstlisting}[caption=window to share arrays, label=code:mpi_window,style=c]
MPI_Win_create(A, len1, sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &winA);
MPI_Win_create(B, len2, sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &winB);
\end{lstlisting}

Afterwards other processes can read from them as presented in \hyperref[code:mpi_get]{Listing \ref*{code:mpi_get}}.

\begin{lstlisting}[caption=window to share arrays, label=code:mpi_get,style=c]
int value;
MPI_Win_lock\(MPI_LOCK_SHARED, targetRank, 0, win1\);
  MPI_Get\(value, 1, MPI_INT, targetRank, d, 1, MPI_INT, win1\);
MPI_Win_unlock\(targetRank, win1\);
\end{lstlisting}

Eventually we wrote a function getValueFrom(int index,MPI\_Win win,...) for reading elements from a certain position in $A$ (or $B$).
Therefor it calculates which process has the required part of $A$ (or $B$).
If it is the same process it reads it from its own array.
Otherwise it uses the access method from \hyperref[code:mpi_get]{Listing \ref*{code:mpi_get}}.

For a MPI version of the corank algorithm, we replaced all occurrences of $A[i]$ and $B[i]$ with theValueFrom()-function.


\textbf{Issues with MPI:}
At the time of writing we had problems to run our MPI implementation with arbitrary array sizes.
So we assume that the number of processes divides the array lengths and that both arrays have the same size.
\section{Testing}

