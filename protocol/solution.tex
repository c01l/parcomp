\section{Solution}
For every solution we took the following approach:
The sorted input arrays $A$ and $B$ are split into parts
$A = A_1 \concat A_2 \concat \ldots \concat A_p$  and
$B = B_1 \concat B_2 \concat \ldots \concat B_p$
, in a way that all elements in $A_i \cup B_i$ are smaller then those in $A_{i+1} \cup B_{i+1}$. 
With such a distribution given, each Process $i$ merges independently $A_i$ and $B_i$ using a simple sequential merge algorithm.
Afterwards a master process concatinates the results.
The sequential merge function is in following noted by $\merge()$.

\subsection{Co-ranking}
In order to obtain a distribution as mentioned above, we implemented a co-ranking algorithm.
The idea is that for a given index $i$ of the resulting array $C$, 
the co-ranking algorithm $\corank(i)$ should yield a pair of indexes $(j,k)$
such that
\begin{equation}\label{eq:coranking1}
  C[0\ldots i-1] = \merge(A[0\ldots j-1],B[0\ldots k-1]).
\end{equation}
The impact for parallelization is the following:
Let $i$ be the id of a process,
then we make this process calculate the entries of $C[i \cdot l, \ldots, (i+1)\cdot l - 1]$,
where $l$ is the blocksize.
Therefore it uses the coranking algorithm to get
$(j_1,k_1) = \corank(i\cdot l)$ and $(j_2,k_2) = \corank((i+1)\cdot l)$.
With \eqref{eq:coranking1} one has
\begin{align*}
  \merge(A[j_1,\ldots,j_2],B[k_1,\ldots,k_2]) = C[i \cdot l,\ldots, (i+1)l - 1].
\end{align*}
So each process can independently both calculate the coranks and perform the merge.

To calculate the coranks we use that
$j$ and $k$, which fulfill \eqref{eq:coranking1}, are also given by the following
properties\footnote{As shown in https://arxiv.org/abs/1303.4312}
\begin{enumerate}
  \item $j + k = i$
  \item $j = 0 \vee A[j-1] \leq B[k]$
  \item $k = 0 \vee B[k-1] < A[j]$
\end{enumerate}
This leads to following implementation in C.
\lstinputlisting[caption=co-ranking algorithm in C,label=code:corank,style=c]{corank.c}


\subsection{Implementation}
Every implementation uses the following structure:
\begin{enumerate}
  \item read the input
  \item merge in parallel using co-ranks
  \item put the parts together
  \item check if result is ordered again
\end{enumerate}
The details for each implementation are presented in the following paragraphs.

\subsubsection{OpenMP}
In OpenMP, the programm runs sequentially in one master thread until it reaches a parallel section.
At this point new threads are created to execute this section simultaneously.
This section is marked by the directive

\textbf{\#pragma omp parallel num\_threads(p) private(a, b, ..)},

where p is the number of threads and a, b, ... are local variables.
All threads share a common address space.
If a variable is not explicitly marked as private, all threads access the same address when they use this variable.
When the parallel section is left, join ... master thread runs again.

For our implementation this means, that the master thread first reads/creates the input arrays.
Afterwards it allocates the space for the resulting array and makes it available in a public variable.
Then the parallel section is entered and the threads are created.
Each thread calculates its coranks for the indexes
$id \cdot \frac{output\_length}{number\_of\_threads}$
and
$(id+1)\cdot \frac{output\_length}{number\_of\_threads}$, where $id$ is the id of the actual thread.
Then it merges directly from the input arrays into the space that is allocated for the result.

Note here that both the output array and the input arrays are public variables for all threads.
Then the parallel segment is left and the master thread checks the result for corectness.

The following snipped shows how the parallel part


\subsubsection{Cilk}

\subsubsection{MPI}
For the MPI implementation it is assumed that the input arrays are already distributed among all processes.
In contrast to other implementation these processes need to activly communicate to calculate the coranks.
To archieve this, each process makes its array parts accessible by opening a MPI window.
This is demonstrated in \hyperref[code:mpi_window]{Listing \ref*{code:mpi_window}}.

\begin{lstlisting}[caption=window to share arrays, label=code:mpi_window,style=c]
MPI_Win_create(A, len1, sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &winA);
MPI_Win_create(B, len2, sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &winB);
\end{lstlisting}

Afterwards other processes can read from them as presented in \hyperref[code:mpi_get]{Listing \ref*{code:mpi_get}}.

\begin{lstlisting}[caption=window to share arrays, label=code:mpi_get,style=c]
int value;
MPI_Win_lock\(MPI_LOCK_SHARED, targetRank, 0, win1\);
  MPI_Get\(value, 1, MPI_INT, targetRank, d, 1, MPI_INT, win1\);
MPI_Win_unlock\(targetRank, win1\);
\end{lstlisting}

Eventually we wrote a function getValueFrom(int index,MPI\_Win win,...) for reading elements from a certain position in $A$ (or $B$).
Therefor it calculates which process has the required part of $A$ (or $B$).
If it is the same process it reads it from its own array.
Otherwise it uses the access method from \hyperref[code:mpi_get]{Listing \ref*{code:mpi_get}}.

For a MPI version of the corank algorithm, we replaced all occurrences of $A[i]$ and $B[i]$ with theValueFrom()-function.


\textbf{Problems with MPI:}
At the time of writing we had problems to run our MPI implementation with arbitrary array sizes.
So we assume that the number of processes divides the array lengths and that both arrays have the same size.
Unfortunately we could only implement a solution for INT-arrays.

\section{Testing}
We tested our solutions with different .


